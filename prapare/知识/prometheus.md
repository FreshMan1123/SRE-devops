Prometheus怎么配置告警规则
"Prometheus告警规则用YAML配置，包含告警名称、表达式、持续时间、标签和注释。表达式用PromQL写，当条件为true时触发告警。常见的有CPU、内存、磁盘使用率告警。"

"你在简历中提到搭建了企业级监控平台，基于Prometheus，告警转发到企业微信，效果效率<1s。
> 能详细说说你们的监控架构是怎么设计的吗？
> 如果告警量很大，怎么避免告警风暴？"
 "我们用的是Prometheus + Alertmanager架构，Alertmanager配置webhook接收器，
> 告警触发时发送HTTP POST请求到我们的程序，JSON格式包含告警名称、级别、标签等信息。
> 我们的程序解析JSON后，根据告警级别（critical/warning）和资源类型（CPU/内存/磁盘）进行路由，
> 然后转换成企业微信机器人消息格式转发。

 告警风暴处理：用group_by按instance和alertname分组，设置5分钟抑制期，
> 同时配置主告警抑制，比如服务器宕机时只发主告警，抑制相关的CPU、内存等子告警。"


请简述 Prometheus 的告警规则（Alerting Rule）配置的主要字段及其作用，
主要配置字段有
1.alert,也就是告警策略的名字
2.expr，也就是告警触发的条件
3.for 告警触发的持续时间
4.labels 告警自定义表情
5.annotaions：注解，表示自定义表述告警描述摘要等

Prometheus
配置方法

普罗米修斯metric指标里边都包含哪些内容？
metric_name{label1="value1", label2="value2"} value timestamp
1.指标名称
2.标签
3.值
3.时间戳

指标类型：
计数器:递增指标，顾名思义，它只可以增加或者归零，不可减少，常用于对http请求进行计数等。
仪表盘： 实时数据，比如说cpu，内存，磁盘利用率等等
直方图：常用于表示统计一组样本的分布情况，比如延迟。
摘要:直接获取分位数，如特定某个时刻的延迟

promehteus在生产环境需要常关注的指标大概有哪些
在k8s环境下，一般习惯分为四层，基础设施层-中间件依赖层-应用层-业务层
对于基础设施层又分为pod层，node层，集群层。pod层关注cpu，内存，磁盘网络，以及pod重启情况。node层关注cpu，内存，磁盘网络情况。集群层关注k8s各组件的存活情况，api server的延迟情况，node的unready情况。
对于中间件依赖层，则是关注 mysql慢查询进程锁查询延迟 查询请求率等情况。redis则是连接池，缓存过期情况。nginx则是请求数以及状态码分布
对于应用层，我们需要关注四大指标，也就是 延迟 错误 请求 饱和度。延迟则是关注p99延迟，队列等待时间。错误关注用户请求的错误状态码情况，超时率，重试成功率，请求则是关注每秒增长的https请求，饱和度则是关注系统资源对应用的瓶颈
对于业务层，则需要关注 用户的体验情况，比如说登陆注册购买成功率以及用户期望服务指标SLA

CPU，磁盘，内存，网络可以进行怎么细化。
1. CPU监控
- CPU使用率：总体使用率、各核心使用率
- CPU负载：1分钟、5分钟、15分钟负载
- CPU突刺：短时间高CPU使用率
- CPU队列：运行队列长度、等待队列长度
- CPU中断：中断频率、软中断、硬中断
- CPU上下文切换：上下文切换频率

2. 内存监控
- 物理内存：总内存、已用内存、可用内存
- 内存使用率：带Cache和不带Cache的使用率
- 交换内存：Swap使用量、Swap使用率
- 内存碎片：内存碎片化程度
- 内存回收：内存回收频率、回收时间
- 内存泄漏：内存增长趋势

3. 磁盘监控
- 磁盘使用率：各分区使用率
- 磁盘IO：读写速度、IOPS、IO等待时间
- 磁盘队列：队列长度、平均等待时间
- 磁盘错误：IO错误、坏道数量
- 磁盘温度：硬盘温度监控

4. 网络监控
- 网络流量：入站流量、出站流量
- 网络连接：TCP连接数、连接状态分布
- 网络延迟：网络延迟、丢包率
- 网络错误：连接错误、超时错误
- 网络带宽：带宽使用率、带宽利用率

Prometheus是如何收集监控数据的？
通过pull模式，周期性抓取

pull模式和push模式的区别
pull模式是监控系统主动从服务暴露的/metrics端口来获取指标数据，优势是能够避免大量目标推送导致阻塞，同时也方便管理调试。但是缺点是依赖于监控系统必须能访问所有目标，同时无法监控瞬时任务
push模式是目标服务主动向监控系统的push gateway发送指标数据，push gateway作为中介接受数据，监控系统拉取数据。优势是只用出站连接，服务器更安全，同时适合短时任务。劣势是大量请求可能导致推送风暴，也无法检测目标服务是否存活

Prometheus的数据存储机制是怎样的？数据保留多久？
本地存储：时间序列数据库，默认保留15天
远程存储：支持InfluxDB、M3DB等
数据压缩：使用高效的时间序列压缩算法

告警转发
一个JSON里可以包含多个告警。
详细解释
Prometheus 的 Alertmanager Webhook 推送数据结构中，alerts 字段是一个数组，每个元素代表一个具体的告警事件。
也就是说：一次推送可以携带一组（一个或多个）告警。
典型场景
如果同一时间有多台主机、多个服务同时触发了同一个告警规则（比如多台Redis实例宕机），Alertmanager 会把它们分组，一起推送到 Webhook。
你收到的 JSON 里，alerts 数组就会有多个元素，每个元素代表一条独立的告警。

我们有 在特定服务器上部署有 告警转发服务器，prometheus 某某Pod触发告警规则后，会向我们指定ip和端口上发送 json，比如说 10.10.10.10：10/normal  或者是 10.10.10.10/normal ，我们是通过对不同api进行发送json来进行告警分级的，程序内部实现了路由转发，同时再根据实例类型，再进行一次分类，根据 实例类型，告警紧急度来发送到不同的企业微信群。同时在告警规则里添加了 org-Owner，我们程序能提取json里面的org-Owner，再通过拼装成 企业微信机器人能接受的 message，实现动态@人

Prometheus是如何发现Kubernetes中的服务并进行数据采集的？
这个的话，在容器内部，我们会暴露containerport，也就是metrics端口，同时配置注解来告知prometheus metrics端口的有无以及在哪个地方。protmehus也需要同步配置一次kubernetes_sd_configs来进行annontions数据的捕获与处理，通过pull的方式主动采集我们的指标。

每个镜像都默认有meterics端口吗？如果没有应该怎么做。
不是的，对于我们自建镜像来说，metrics端口需要我们引入prometheus客户端进行指标采集端口的一个暴露。

假设你的公司使用 Prometheus 监控 Kubernetes 集群。某天，负责业务的同事向你反映，他们的一个核心服务（假设名为 user-service）的 API 延迟突然变得很高。
请问，你会如何使用 Prometheus 和相关的工具来定位这个延迟问题的根源？
1.使用promehteus来查看99%的api延迟，检查究竟是我们大部分请求的api延迟情况以及请求失败的情况。
2.定位到底是一个pod慢，还是全部pod慢，是一个api慢，还是整个服务慢。使用promehteus对pod
实例以及api实例进行一个拆分。
3.使用promehteus查看特定pod的内存，cpu利用率，以及io情况，同步检查集群的内存cpu利用率以及io情况，查看是不是出现资源瓶颈
4. 同步检查下游服务，如果说我们服务连接有mysql，或者redis，那需要进行mysql的慢查询检查以及redis的连接池检查。
5. 除去这些之外，我们还可以到另一台机器上ping服务器，查看报文情况，连接到nginx负载均衡检查流量转发的超时情况，同步排查网络问题
6. 也可以适当检查一下tcp连接数，看是不是tcp链接满了导致请求延迟

场景：你负责维护公司的监控系统，核心是 Prometheus。在一次业务大促期间，Prometheus 自身的内存使用率持续飙升，马上就要 OOMKilled 了。根据经验，你怀疑是“高基数（High Cardinality）”问题导致的。
问题：
请用自己的话解释一下，在 Prometheus 中，到底什么是“高基数”？为什么它会对 Prometheus 造成致命影响？
高基数是指标用了太多独一无二的标签种类，比如说user_id,请求id等，因为promehteus在工作时，会为每个标签（行）的种类在内存里进行单独维护，比如说状态码（200，400，404）就是一个单独维护的账本，如果说我们使用了过于离散的数据作为标签的话，就可能会导致要维护的账本过多，内存压力暴增，出现OOMKILLER 

rate() 和 increase() 函数都是用来计算 Counter 类型指标增长速率的，它们之间有什么核心区别？
rate是每秒平均的增长速率
increase()是整个实际范围内总增长量

告警有哪几种处理方式：
1.确认（acknowledge），表示收到了正在看，告警本身仍然是激活状态
2.解决，表示告警已经被处理，可以关闭了
3.静默，表示告警已经知晓，但这是预期中的或者暂时不重要的，让其一段时间内不再通知

promtheus有哪两种写入方案
1.远程写入：需要部署virtermicrs等时序性数据库来存储拉取到的数据。优点是可扩展性强，适合查询以及存储，缺点是相对复杂
2.联邦采集：也就是根节点中心promehteus通过match的方式聚合其余叶节点prometheus采集到的数据，它的优势是灵活内置，缺点是不适合作为一个长期存储

如果你发现应用的P99延迟突然从100ms飙升到2秒，但P50延迟还是100ms，你会如何排查这个问题？请说出具体的排查步骤和可能的原因。
p99表示有1%的用户延迟超过2s，而有50%的请求延迟为100ms。对于这种情况的可能是
1. jvm进行内存回收时，GC停顿导致的应用响应延迟
2. 网络抖动
3. 资源不足
4. 业务量增大
5. mysql慢查询，redis连接池
6. tcp连接数不够。time_wait等待
7. 缓存失效，比如说缓存雪崩缓存击穿缓存穿透等，导致连接达到数据库，查询效率变慢


现在我们继续下一道题。你们公司的Prometheus监控系统最近告警频繁，运维同事反馈说收到了很多target down的告警，但是检查后发现对应的服务实际上是正常运行的。
你作为SRE需要排查这个问题。请描述你的排查思路，以及可能的原因有哪些？
1. 确认是哪些target显示down
2. 检查prometheus配置，查看持续出发时间，for参数设置
3. 检查网络连通性，测试target连通
4. 确认target地址是否正确
5. 查看promehteus日志

既然你提到了告警持续时间，那我问你：在Prometheus告警规则中，for参数设置为多长时间比较合适？如果设置为0会有什么问题？
另外，如果你发现是网络波动导致的，除了调整告警规则，还有哪些技术手段可以解决这个问题？
for参数设置一般需要根据具体的业务以及告警紧急程度来设置，一般需要长期占用某个资源的业务就设置长一点，普通告警设置三分钟，紧急设置三十秒。设置为0会导致一触发就告警，容易告警疲劳/
如果是因为网络波动导致的，
1.那可以设置多promtheus的高可用架构，每个prometheus同步采集target的指标，同时需要设置对应的告警去重机制防止告警重复
2. 可以配置scrape，增加采集的超时时间
3. 配置告警抑制机制，避免因为网络波动导致的牵连告警。

为什么网络波动会导致promehteus告警
因为网络波动时，可能导致数据包丢失，tcp重传；或者是连接超时；甚至是连接中断。此时prometheus在规定的超时时间内没有采集到指标数据，默认服务挂了，触发告警

p99延迟是什么
它衡量的是99%的请求都低于的那个延迟时间。可以用于衡量任何请求的耗时，无论是用户到服务器端对端的延迟还是访问某服务的延迟还是访问某api的延迟

garafna有哪些常用的运维指标
1.延迟，2.RPS/QPS每秒请求数，3.4xx，5xx错误率 4.各种资源利用率

 RPS/QPS是什么
 RPS是每秒服务器处理的请求数，包括http请求或者api网关等。
 QPS是每秒服务处理的查询数，一般用于数据库的查询，衡量数据库的读性能

我想要监控redis和mysql该如何监控呢
对于非云原生情况下，我们可以用redis exporter或者是mysql exporter来进行监控
对于云原生环境下的话，我们可以用InnoDBCluster，它会自动帮我们采集mysql指标，而对于redis来说，我们可以使用deployemnt部署一下redis-exporter

promehteus高效查询是怎么实现的
1. 按照时间戳分块存储
2， 倒排索引，由标签定求交集得时间序列。
3. 多层缓存+数据压缩
4. 并发控制，允许多个时间序列并行处理
